{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By Viraj A A Sanghai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pandas package\n",
    "import pandas\n",
    "price_data = pandas.read_csv(\"Downloads/data_input_sample/stock_prices.csv\")\n",
    "#View stock prices data\n",
    "price_data.head(5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View full stock prices data\n",
    "#price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group stock prices by sector\n",
    "price_data['sector'].value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group stock prices by industry\n",
    "price_data['industry'].value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the different prices taken by the stocks\n",
    "price_data['adjusted_price'].value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values in the stock prices data\n",
    "price_data['adjusted_price'].isnull().sum();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the length of time series for each security and finding the number of securities which is 529.\n",
    "price_data['security_id'].value_counts();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Features data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the source features data\n",
    "source_features = pandas.read_csv(\"Downloads/data_input_sample/source_features.csv\")\n",
    "#print(source_features.head(5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the length of time series for source features\n",
    "source_features['source_id'].value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index range of source features\n",
    "source_features.index;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After looking at the different data sets, we choose to focus on the source features. \n",
    "#We try to process it in a form that makes it more useable\n",
    "import numpy as np\n",
    "#As suggested in the guidelines, we reduce the 23,043 time series by grouping the data using category 4 and the unique report date\n",
    "source_cat4 = source_features.groupby(['category_4','ReportDate']).agg({'source_id': np.sum,'source_features':sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View sample of data - There are about 1387 category ids\n",
    "source_cat4.head(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_cat4.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are left with about 8003 time series with unique source ids and report dates\n",
    "source_cat4['source_id'].value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since almost all securities prices data have time series with more than 500 points, \n",
    "#we keep only those time series source ids in the sourcecat4 data\n",
    "boo_source = ((source_cat4['source_id'].value_counts())>=500) \n",
    "boo_source;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We see that this will reduce our 8003 unique source ids to just 988\n",
    "((source_cat4['source_id'].value_counts())>=500) .value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can double check this\n",
    "boo_check = boo_source[boo_source.values == True]\n",
    "boo_check;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can isin to filter out the source ids that we require from the source_cat4 data \n",
    "source_cat4 = source_cat4[(source_cat4['source_id']).isin(boo_check.index.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the source_cat4 data\n",
    "source_cat4.head(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can check the length of the time series of the left over data points\n",
    "source_cat4['source_id'].value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again we can create groups by source ids and use  that as the main identifier for our time series\n",
    "source_fin = source_cat4.groupby(['source_id'])\n",
    "source_fin;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the adfuller test to check for unit roots and stationarity of the 988 time series we have left. \n",
    "#We find we have 56 non-stationary time series. We plot them and count the the stationary and non statiopnary values in dictionaries.\n",
    "#Although it is an extreme measure we use differencing to transform some of those time series to stationary data.\n",
    "#After differencing once we are left with 9 non-stationary time series. After differencing twice we are left with 2 non-stationary time series.\n",
    "#After differencing a third time we are left with one non-statioanry time series. \n",
    "#We drop the last remaining time series\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "source_stat = dict()\n",
    "source_stat['stationary'] = 0\n",
    "source_stat['not stationary'] = 0\n",
    "source_diffstat = dict()\n",
    "source_diffstat['diff stationary'] = 0\n",
    "source_diffstat['diff not stationary'] = 0\n",
    "source_diff2stat = dict()\n",
    "source_diff2stat['diff2 stationary'] = 0\n",
    "source_diff2stat['diff2 not stationary'] = 0\n",
    "source_diff3stat = dict()\n",
    "source_diff3stat['diff3 stationary'] = 0\n",
    "source_diff3stat['diff3 not stationary'] = 0\n",
    "not_stat  = []\n",
    "for name, group in source_fin:\n",
    "    #print(name)\n",
    "    #print(group['source_features'])\n",
    "    results= adfuller(group['source_features'])\n",
    "    if results[1]<=0.05:\n",
    "        #print(results[1], 'stationary')\n",
    "        source_stat['stationary']+=1 \n",
    "    else:\n",
    "        #print(results[1], 'not stationary')\n",
    "        source_stat['not stationary']+=1\n",
    "        #(group['source_features']).plot()\n",
    "        #plt.show()\n",
    "        not_stat.append(name)\n",
    "        #source_fin.drop(index = name)\n",
    "        \n",
    "       \n",
    "            \n",
    "\n",
    "    #(group['source_features']).plot()\n",
    "    #print(results)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stationarity statistic of time series before differencing\n",
    "source_stat;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stationarity statistic of time series after differencing once on non stationary data\n",
    "not_stat;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stationarity statistic of time series after differencing twice on non stationary data\n",
    "source_diff2stat;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stationarity statistic of time series after differencing thrice on non stationary data\n",
    "source_diff3stat;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few checks in the next few inputs\n",
    "#source_features['category_4'].value_counts()\n",
    "#source_cat4.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_cat4['source_features'][0]['2017-02-01 00:00:00 ':'2017-02-05 00:00:00 '];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_fin.head(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_fin['source_id'].value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(source_fin);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_fin.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_fin.get_group(107140)\n",
    "#Source id no 22416 is the one still with non-stationary data. \n",
    "source_fin.get_group(22416);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_cat4.index.get_level_values(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a new dataframe using all our groups to keep only the essential data\n",
    "#The report ids are the row indices, the source ids are the column labels \n",
    "#and the values are the values for the corresponding source features\n",
    "final_source_df = pandas.DataFrame()\n",
    "\n",
    "for name, group in source_fin:\n",
    "    #print(name)\n",
    "    #print(group['source_features'])\n",
    "    #print(group.index.get_level_values(0))\n",
    "    new_df = pandas.DataFrame({name: group['source_features'] })\n",
    "    new_df.index = new_df.index.droplevel(0)\n",
    "    #print(new_df)  \n",
    "    #final_source_df = pandas.concat([final_source_df,new_df],axis=0)\n",
    "    final_source_df = final_source_df.join(new_df,how = 'outer')\n",
    "    #final_source_df = final_source_df.drop_duplicates()\n",
    " \n",
    "#pandas.set_option('display.max_columns',1000)\n",
    "#print(final_source_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_source_df[18].value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because we did an outer join, there are a few repeated rows which we can recombine\n",
    "new_fin_df = final_source_df.groupby(level=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(new_fin_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We drop the non-stationary data column\n",
    "new_fin_df.drop(not_stat, axis = 1, inplace = True)\n",
    "#print(new_fin_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We only keep rows that have 500 values\n",
    "source_new = new_fin_df.dropna(thresh = 500)\n",
    "#print(source_new);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We throw away columns that have all NaN values\n",
    "source_new = source_new.dropna(axis=1,how ='all')\n",
    "#print(source_new);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We interpolate the data to fill the NaN values with the mean of the columns\n",
    "source_new2 = source_new.fillna(source_new.mean())\n",
    "#print(source_new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data.head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "metal_df = price_data[price_data['name'] == price_data['name'][0]]\n",
    "#print(metal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "metals_returns = metal_df[['adjusted_price','effective_date']]\n",
    "metals_returns['adjusted_price'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "metals_returns.set_index('effective_date',inplace=True)\n",
    "#metals_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virajsanghai/python3/lib/python3.6/site-packages/pandas/core/indexing.py:621: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "metals_returns.loc[:,'adjusted_price'] = metals_returns['adjusted_price'].pct_change().values\n",
    "#metals_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "metals_returns = metals_returns.dropna(axis=0)\n",
    "metals_returns.rename(columns = {'adjusted_price':'returns'}, inplace=True) \n",
    "#metals_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_new2.index;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "metals_returns.index;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "select_dates = pd.date_range(start = source_new2.index[0], end=source_new2.index[-1], freq ='B')\n",
    "#np.array(select_dates)\n",
    "sel = select_dates.strftime('%Y-%m-%d')\n",
    "#sel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virajsanghai/python3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "metals_returns_fin = metals_returns.loc[sel] #Use reindex instead\n",
    "#metals_returns_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "metals_returns_fin.fillna(value = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First method: stationarity, feature scaling, lin regression, elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First split full data, then feature scale by fitting training data, then transforming test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(source_new2, metals_returns_fin, test_size = 0.2, random_state=0)\n",
    "X_train, X_test = np.split(source_new2, [int(0.8*len(source_new2))])\n",
    "y_train, y_test = np.split(metals_returns_fin, [int(0.8*len(metals_returns_fin))])\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train.loc[:] = scaler.fit_transform(X_train)\n",
    "X_test.loc[:] = scaler.transform(X_test)\n",
    "#X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(metals_returns_fin.isnull());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check met returns are stationary\n",
    "met_result= adfuller(metals_returns_fin['returns'])\n",
    "met_result[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metals_returns_fin['returns'].plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lin regression in statsmodel, can also use sklearn, make sure to add constant for statsmodels, \n",
    "#sklearn has a separate intercept and doesn't need a constant, sklearn better for metrics \n",
    "import statsmodels.api as sm\n",
    "#print(hist_data.head())\n",
    "#results2 = sm.OLS(y_train,X_train).fit_regularized(method='elastic_net',alpha=0.1,L1_wt = 0.5,refit = True)\n",
    "#print([\"{0:.15f}\".format(i) for i in results2.params])\n",
    "#print(results2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose best alpha and l1 ratio for elastic net based on r^2\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "def build_model(_alpha, _l1_ratio):\n",
    "    estimator = ElasticNet(\n",
    "        alpha=_alpha,\n",
    "        l1_ratio=_l1_ratio,\n",
    "        fit_intercept=True,\n",
    "        normalize=False,\n",
    "        precompute=False,\n",
    "        max_iter=1000,\n",
    "        copy_X=True,\n",
    "        tol=0.1,\n",
    "        warm_start=False,\n",
    "        positive=False,\n",
    "        random_state=None,\n",
    "        selection='random'\n",
    "    )\n",
    "\n",
    "    return MultiOutputRegressor(estimator, n_jobs=1)\n",
    "    #return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use time series split to maintain temporal order\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(source_new2, metals_returns_fin, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 125 out of 125 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "#Use gridsearch CV to find best parameters\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'estimator__alpha':(0.1, 0.3, 0.5, 0.7, 0.9),\n",
    "    'estimator__l1_ratio':(0.1, 0.3, 0.5, 0.7, 0.9)\n",
    "}\n",
    "\n",
    "#for i in range(100):\n",
    "model = build_model(_alpha=1.0, _l1_ratio=0.3)\n",
    "\n",
    "finder = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=params,\n",
    "    scoring='r2',\n",
    "    #fit_params=None,\n",
    "    n_jobs=None,\n",
    "    iid=False,\n",
    "    refit=False,\n",
    "    cv=tscv,  # change this to the splitter subject to test\n",
    "    verbose=1,\n",
    "    pre_dispatch=8,\n",
    "    #error_score=-999,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "finder.fit(X_train, y_train)\n",
    "\n",
    "best_params = finder.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res = finder.cv_results_\n",
    "final_res;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply best fit model to data. Test using different metrics including r^2\n",
    "from sklearn.metrics import r2_score\n",
    "# optimal model\n",
    "model = ElasticNet(\n",
    "        alpha=0.1,\n",
    "        l1_ratio=0.5,\n",
    "        fit_intercept=True,\n",
    "        normalize=False,\n",
    "        precompute=False,\n",
    "        max_iter=1000,\n",
    "        copy_X=True,\n",
    "        tol=1e-4,\n",
    "        warm_start=False,\n",
    "        positive=False,\n",
    "        random_state=None,\n",
    "        selection='cyclic'\n",
    "    )\n",
    "# train model\n",
    "model.fit(X_train, y_train)\n",
    "# test score\n",
    "y_predicted = model.predict(X_test)\n",
    "score = r2_score(y_test, y_predicted, multioutput='uniform_average')\n",
    "#print(\"Test Loss: {0:.15f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print([\"{0:.15f}\".format(i) for i in model.coef_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: stationarity, feature scaling, PCA, lin regression, elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For PCA first centre all data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "source_new2.iloc[:,0:128] = scaler.fit_transform(source_new2.iloc[:,0:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure sample size is greater than number of features\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(source_new2.iloc[:,0:128])\n",
    "#print((pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.shape;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate principal components explain 90% variance\n",
    "sum(np.cumsum(pca.explained_variance_ratio_)<=0.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.9, random_state=None,\n",
       "    svd_solver='full', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another way to keep 90% variance of components\n",
    "from sklearn.decomposition import PCA\n",
    "pca_req = PCA(n_components=0.9,svd_solver='full')\n",
    "pca_req.fit(source_new2.iloc[:,0:127])\n",
    "#print((pca_req.components_.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalculate transformed design matrix using principal eigenvectors\n",
    "W = np.dot(source_new2.values[:,0:127], pca_req.components_.T)\n",
    "W;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(source_new2, metals_returns_fin, test_size = 0.2, random_state=0)\n",
    "W_train, W_test = np.split(W, [int(0.8*len(source_new2))])\n",
    "y_train, y_test = np.split(metals_returns_fin, [int(0.8*len(metals_returns_fin))])\n",
    "#y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use linear regression first\n",
    "import statsmodels.api as sm\n",
    "W_train_stats = sm.add_constant(W_train)\n",
    "#print(hist_data.head())\n",
    "results3 = sm.OLS(y_train,W_train_stats).fit()\n",
    "#print(results3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virajsanghai/python3/lib/python3.6/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "#sklearn linear regression better for metrics and future predictions\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, accuracy_score, f1_score\n",
    "regr = linear_model.LinearRegression()\n",
    "results4 = regr.fit(W_train,y_train)\n",
    "#print(regr.coef_)\n",
    "y_predict1 =regr.predict(W_test)\n",
    "score_lin_r = r2_score(y_test, y_predict1, multioutput='uniform_average')\n",
    "#print(\"Test Loss: {0:.15f}\".format(score_lin_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(regr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate accuracy and f1 score by times returns are positive or negative\n",
    "bin_test = y_test > 0\n",
    "bin_predict = y_predict1 > 0\n",
    "score_lin_f1 = f1_score(bin_test, bin_predict)\n",
    "#print(\"F1: {0:.15f}\".format(score_lin_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lin_acc = accuracy_score(bin_test, bin_predict)\n",
    "#print(\"acc: {0:.15f}\".format(score_lin_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(bin_test == bin_predict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate daily sharpe ratio against some benchmark and annualize it\n",
    "sharpe_daily = np.mean(y_predict1 - 0)/np.std(y_predict1-0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(260)*sharpe_daily;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Elastic Net again, alpha = 1, l1 = 0, only ridge regression, l1 = 1 is lasso, l1 <0.01 not reliable\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "def build_model(_alpha, _l1_ratio):\n",
    "    estimator = ElasticNet(\n",
    "        alpha=_alpha,\n",
    "        l1_ratio=_l1_ratio,\n",
    "        fit_intercept=True,\n",
    "        normalize=False,\n",
    "        precompute=False,\n",
    "        max_iter=1000,\n",
    "        copy_X=True,\n",
    "        tol=0.1,\n",
    "        warm_start=False,\n",
    "        positive=False,\n",
    "        random_state=None,\n",
    "        selection='random'\n",
    "    )\n",
    "\n",
    "    return MultiOutputRegressor(estimator, n_jobs=1)\n",
    "    #return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(source_new2, metals_returns_fin, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 180 out of 180 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'estimator__alpha':(0.1, 0.3, 0.5, 0.7, 0.9,1),\n",
    "    'estimator__l1_ratio':(0.1, 0.3, 0.5, 0.7, 0.9,1)\n",
    "}\n",
    "\n",
    "#for i in range(100):\n",
    "model = build_model(_alpha=1.0, _l1_ratio=0.3)\n",
    "\n",
    "finder2 = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=params,\n",
    "    scoring='r2',\n",
    "    #fit_params=None,\n",
    "    n_jobs=None,\n",
    "    iid=False,\n",
    "    refit=False,\n",
    "    cv=tscv,  # change this to the splitter subject to test\n",
    "    verbose=1,\n",
    "    pre_dispatch=8,\n",
    "    #error_score=-999,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "finder2.fit(W_train, y_train)\n",
    "\n",
    "best_params2 = finder2.best_params_\n",
    "best_params2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# optimal model\n",
    "model2 = ElasticNet(\n",
    "        alpha=0.1,\n",
    "        l1_ratio=0.05,\n",
    "        fit_intercept=True,\n",
    "        normalize=False,\n",
    "        precompute=False,\n",
    "        max_iter=1000,\n",
    "        copy_X=True,\n",
    "        tol=1e-4,\n",
    "        warm_start=False,\n",
    "        positive=False,\n",
    "        random_state=None,\n",
    "        selection='cyclic'\n",
    "    )\n",
    "# train model\n",
    "model2.fit(W_train, y_train)\n",
    "# test score\n",
    "y_predict_pca = model2.predict(W_test)\n",
    "score2 = r2_score(y_test, y_predict_pca, multioutput='uniform_average')\n",
    "#print(\"Test Loss: {0:.15f}\".format(score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print([\"{0:.15f}\".format(i) for i in model2.coef_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_test = y_test > 0\n",
    "bin_predict_pca = y_predict_pca > 0\n",
    "score_el_f1 = f1_score(bin_test, bin_predict_pca)\n",
    "#print(\"F1: {0:.15f}\".format(score_el_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_el_acc = accuracy_score(bin_test, bin_predict_pca)\n",
    "#print(\"acc: {0:.15f}\".format(score_el_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_daily2 = np.mean(y_predict_pca - 0)/np.std(y_predict_pca-0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(260)*sharpe_daily2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "#print(hist_data.head())\n",
    "results5 = sm.OLS(y_train,W_train_stats).fit_regularized(method='elastic_net',alpha=0.1,L1_wt = 0,refit = True)\n",
    "#print([\"{0:.15f}\".format(i) for i in results5.params])\n",
    "#print(results5.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
